---
title: "Worksheet 13: Introduction to Classification"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r global_options, include=FALSE}
# The following code is a basic setup of options for your document
# You won't have to edit it (unless you want to!)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE,  
                      warning = FALSE, message = FALSE, 
                      fig.align = "center",
                      R.options = list(max.print = 100))

# Edit the file starting below
```

## 1. Set up

We will use the packages `tidyverse` and `plotROC`.

```{r, eval=FALSE}
# Install new packages (only needed once!)
install.packages("plotROC")
```

```{r, message = FALSE}
# Load packages
library(tidyverse)
library(plotROC) 
```

Let's consider the `biopsy` dataset that contains information about tumor biopsy results. Nine features of the tumor were measured (on a 1-10 scale) as well as the `outcome` variable (malignant vs. benign).

```{r}
# Upload the data from GitHub
biopsy <- read_csv("https://raw.githubusercontent.com/laylaguyot/datasets/main//Biopsy.csv")

# Take a quick look at 10 random rows
sample_n(size = 10, biopsy)
```

We will focus on predicting the `outcome` based on `clump_thickness`.

#### **Try it! Use `ggplot` to represent the distribution of `clump_thickness` for malignant and benign tumors. Does there seem to be a relationship?**

```{r}
# Write code here
biopsy %>%
  ggplot(aes(x = clump_thickness, fill = outcome)) +
  geom_histogram(position = "dodge")
```

## 2. Basic classifications

Let's try different ways to classify a tumor as malignant or benign.

### a. Random classification

Let's consider that we would like to predict the outcome randomly. For example, a tumor with any value of `clump_thickness` would have equal probability of being malignant or benign.

```{r}
# Create a predicted variable: sample with replacement
biopsy_pred <- biopsy %>%
  select(clump_thickness, outcome) %>% 
  mutate(predicted = sample(c("malignant","benign"), 
                            size = length(outcome), 
                            replace = TRUE)) 

# Take a look at the predicted variable
head(biopsy_pred)
```

Let's compute the accuracy of our predictions: how many observations were correctly identified as malignant or benign? Why does it make sense?

```{r}
# Accuracy
mean(biopsy_pred$outcome == biopsy_pred$predicted) 
```

Let's represent the distribution of randomly predicted malignancy across values of `clump_thickness`.

```{r}
# Distribution of predicted malignancy across clump_thickness
ggplot(biopsy_pred, aes(y = clump_thickness, fill = predicted)) + 
  geom_boxplot() + 
  scale_x_discrete( )
```

It looks like the distribution of malignant/benign is about the same, regardless of the values of `clump_thickness`. Not a good classification!

### b. Classification based on 1 variable

Most classifiers are based on numeric scores representing the strength of predictions: The higher the score, the more likely the case is a \"success\".

We think that higher values of `clump_thickness` seem to indicate than the tumor is oftentimes malignant rather than benign. Let's classify all tumors with a high value of `clump_thickness` (greater than 9) as malignant.

```{r}
# Create another predicted variable: based on clump_thickness > 9 
biopsy_pred <- biopsy %>%
  select(clump_thickness, outcome) %>% 
  mutate(predicted = ifelse(clump_thickness > 9, 
                            "malignant", "benign")) 

# Take a look at the new predicted variable
head(biopsy_pred)
```

Represent the distribution of `clump_thickness` depending on the new predicted outcome:

```{r}
# Distribution of clump_thickness across predicted malignancy
ggplot(biopsy_pred, aes(y = clump_thickness, fill = predicted)) + 
  geom_boxplot()
```

Has the accuracy improved?

```{r}
# Accuracy
mean(biopsy_pred$outcome == biopsy_pred$predicted) 
```

### c. Cutoff

We chose a cutoff value of 9 for `clump_thickness` in the previous section. But what if we had chosen a different value? Let's calculate the accuracy of our prediction based on different cutoff values. We'll use a `for`-loop to repeat the process.

```{r}
# Initialize vector for accuracy values
accuracy <- vector()

# Define possible cutoff values: from min to max clump thickness
cutoff <- min(biopsy$clump_thickness):max(biopsy$clump_thickness)

# For each cutoff value:
# create a predicted variable and find the resulting accuracy
for(i in cutoff){
  biopsy_pred <- biopsy %>% 
  mutate(predicted = ifelse(clump_thickness > i, 
                            "malignant", "benign")) 
  
  accuracy[i] <- mean(biopsy_pred$outcome == biopsy_pred$predicted)  
}
accuracy
```

Let's represent the accuracy for each cutoff value:

```{r}
# To use ggplot, we need accuracy to be considered as a data frame
ggplot(as.data.frame(accuracy), aes(x = 1:10, # x-axis are values of clump thickness
                                    y = accuracy)) + 
  geom_point() + geom_line() + 
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Cutoff values for clump thickness to determine malignancy")
```

#### **Try it! For which cutoff value of `clump_thickness`, is the accuracy of identifying malignancy the highest? Find the corresponding accuracy.**

```{r}
# Write code here
accuracy[5]
  
```

We found a value that optimized the accuracy!

## 3. Metrics

Let's consider the classifier based on clump thickness greater than 5.

```{r}
# Make predictions when the cutoff value is 5 for clump_thickness
biopsy_pred <- biopsy %>%
  select(clump_thickness, outcome) %>% 
  mutate(predicted = ifelse(clump_thickness > 5, "malignant", "benign"))
```

We should also consider other metrics for evaluating our classification such as true positive rate (TPR) and true negative rate (TNR), also called sensitivity and specificity, respectively.

```{r}
# To make code less lengthy, consider these two vectors
outcome <- biopsy_pred$outcome
predicted <- biopsy_pred$predicted

# Confusion matrix: compare true to predicted condition
table(predicted, outcome) %>% addmargins
```

#### **Try it! If sensitivity represents the true positive rate (truly predicted positive cases over number of positive cases), what is the value of sensitivity? If specificity represents the true negative rate (truly predicted negative cases over number of negative cases), what is the value of specificity?**

```{r}
# sensitivity
163 / 239

# specificity. 1 - specificity is false negative
424 / 444
```

And what is accuracy? The proportion of true positives plus the proportion of true negatives:

```{r}
# Proportion of malignant times TPR
239/683 * mean(predicted[outcome =="malignant"] == "malignant") + # plus
  # Proportion of benign times TNR
  444/683 * mean(predicted[outcome == "benign"] == "benign") 
```

There is a trade-off between sensitivity and the specificity.

(back to slides)

## 4. ROC/AUC

### a. Receiver Operating Characteristics (ROC) curves

A ROC curve represents the trade-off between sensitivity and the specificity for different cutoff values. Usually, the false positive rate (called FPR and representing 1 - TNR) is represented on the x-axis and the true positive rate is represented on the y-axis.

```{r}
# Plot ROC depending on values of clump_thickness to predict the outcome
ROC <- ggplot(biopsy_pred) + 
  geom_roc(aes(d = outcome, m = clump_thickness), n.cuts = 10)
ROC

```

When the cutoff for `clump_thickness` is 10, you get 0 TPR and 0 FPR (nothing predicted malignant). When the cutoff is 0, you get 1 TPR and 1 FPR (everything predicted malignant). What about when the cutoff is 5?

### b. Area under the curve (AUC)

The area under the curve (AUC) quantifies how well our classification is predicting the outcome.

```{r}
# Calculate the area under the curve with function calc_auc()
calc_auc(ROC)
```

### c. What does it mean?

Let's randomly select 2 patients, one with a malignant tumor and one with a benign tumor. We will compare their clump thickness:

-   if the clump thickness was higher for the patient with a malignant tumor, we assign a probability of 1,

-   if the clump thickness was the same for the two patients, we assign a probability of 0.5,

-   if the clump thickness was lower for the patient with a malignant tumor, we assign a probability of 0.

Then we repeat that process 1,000 times.

```{r}
# Replicate the process 
probs <- replicate(1000,{
  
  # Sample 1 patient with malignant tumor
  rand_positive <- biopsy %>%
    filter(outcome == "malignant") %>%
    select(clump_thickness) %>%
    sample_n(size = 1) %>% pull
  
  # Sample 1 patient with benign tumor
  rand_negative <- biopsy %>%
    filter(outcome == "benign") %>%
    select(clump_thickness) %>%
    sample_n(size = 1) %>% pull
  
  # Assign a probability value
  case_when(rand_positive > rand_negative ~ 1, 
            rand_positive == rand_negative ~ .5, 
            rand_positive < rand_negative ~ 0)
})

# AUC
mean(probs)
```

You can interpret the AUC as the fact that a randomly selected patient with a malignant tumor has a higher predicted probability to have a malignant tumor than a randomly selected person with a benign tumor. On average, about 91% of the time, malignant tumors will have higher probabilities of being malignant compared to benign tumors. In a nutshell: the higher the AUC, the better the classifier is!

## Your turn!

You'll practice a basic classification for the `Legendary` status of pokemons:

```{r}
# Upload data from GitHub
pokemon <- read_csv("https://raw.githubusercontent.com/laylaguyot/datasets/main//pokemon.csv")

# Take a look 
head(pokemon)
```

1.  Pick one of the numeric variables in the dataset. Visualize the relationship with `Legendary` status. Post your plot on the following slideshow: <https://docs.google.com/presentation/d/10ZzgjrhoxVXOrq4chY7USeXYb_ZMxn0hSalqRa3fzac/edit?usp=sharing>

```{r}
# Try it!
pokemon %>%
  ggplot(aes(x = Legendary, y = Total)) + 
  geom_bar(stat = "summary", fun = "mean")
```

2.  How good is your numeric variable for this classification? Build a ROC plot with the numeric variable you picked. What is the value of the AUC? Post your ROC plot on the same slide and report the value of the AUC in the comments: <https://docs.google.com/presentation/d/10ZzgjrhoxVXOrq4chY7USeXYb_ZMxn0hSalqRa3fzac/edit?usp=sharing>

```{r}
# Try it!
pokemon_pred <- pokemon %>%
  select(Total, Legendary) %>% 
  mutate(predicted = ifelse(Total > 450, TRUE, FALSE))

pokROC <- ggplot(pokemon_pred) + 
  geom_roc(aes(d = Legendary, m = Total), n.cuts = 10)

pokROC

calc_auc(pokROC)

```

3.  Create a simple classification for the `Legendary` status based on your numeric variable. Choose a cutoff value that maximizes the TPR but also minimizes the FPR (based on your ROC plot). What is the accuracy of your classification?

```{r}
# Try it!
legendary <- pokemon_pred$Legendary
predicted <- pokemon_pred$predicted
table(predicted, legendary) %>% addmargins

# sensitivity
65 / 65
  
# specificity
344 / 735

65/800 * mean(predicted[outcome == TRUE] == TRUE) + # plus
  # Proportion of benign times TNR
735/800 * mean(predicted[outcome == FALSE] == FALSE) 

```
