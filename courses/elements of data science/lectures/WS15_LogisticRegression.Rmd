---
title: "Worksheet 15: Logistic Regression"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r global_options, include=FALSE}
# The following code is a basic setup of options for your document
# You won't have to edit it (unless you want to!)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE,  
                      warning = FALSE, message = FALSE, 
                      fig.align = "center",
                      R.options = list(max.print = 100))

# Edit the file starting below
```

## 1. Set up

We will use the packages `tidyverse` and `plotROC`.

```{r, message = FALSE}
# Load packages
library(tidyverse)
library(plotROC) 
```

We will continue making predictions using the built-in R object called `mtcars`:

```{r}
# Recall this dataset
head(mtcars)

# Note: Transmission am (0 = automatic, 1 = manual)
```

We will try to predict the type of transmission `am` (response variable) based on other features (predictor variable(s)) of a car.

## 2. Predicting a binary response with a numeric predictor

First, let's try to predict the type of transmission `am` based on the fuel consumption of a car `mpg`.

### a. Exploring relationships

In the previous worksheet, we already looked at the relationship between these two variables. Let's take a look again:

```{r}
# Represent the relationship 
ggplot(mtcars, aes(fill = as.factor(am), x = mpg)) +
  geom_boxplot() + scale_y_discrete() + 
  labs(x = "Fuel consumption (mpg)", fill = "Transmission")
```

Does there seem to be a relationship between the fuel consumption and type of transmission?

### b. Fitting a model

We can consider a logistic regression model with `am` as the response variable and `mpg` as the predictor variable:

```{r}
# A little different from the linear model!
ggplot(mtcars, aes(x = mpg, y = am)) + 
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, 
              method.args = list(family = "binomial"))
```

The logistic regression model shows that the probability of the transmission being automatic or manual. We fit a logistic regression model with the function `glm()` (glm stands for generalized linear models) and the argument `family = "binomial"`:

```{r}
# Fit the model (general linear model)
fit_log <- glm(am ~ mpg, data = mtcars, family = "binomial")

# Take a look at the model summary
summary(fit_log)
```

The output gives the logit-form of the model which is:

$\ln{\frac{\hat{p}}{1-\hat{p}}} = -6.6035 + 0.3070 * mpg$

where $\hat{p}$ is the probability of the transmission being manual (the value of 1).

Then we can predict the probability of the transmission being manual by using the probability form:

$\hat{p} = \frac{e^{-6.6035 + 0.3070 * mpg}}{1 + e^{-6.6035 + 0.3070 * mpg}}$

### c. Making predictions

Let's use the expression of the model to make predictions.

#### **Try it! Use (an adjusted R version of) the expression of the probability form to create a new variable called `predictions` that predicts the probability of the transmission being manual based on values of `mpg`. Based on these probabilities, how do we decide if the transmission should be considered automatic or manual?**

```{r}
# Write code here
mtcars %>%
  mutate(predictions = exp(-6.6035 + 0.3070 * mpg) / (1 + exp(-6.6035 + 0.3070 * mpg))) %>%
  select(mpg, am, predictions)
```

Or using the `predict()` function with the `type = "response"` argument (much more convenient):

```{r}
# Calculate a predicted value
mtcars %>% 
  mutate(predictions = predict(fit_log, type = "response")) %>%
  select(mpg, am, predictions)
```

Let's represent our predicted values of transmission depending on `mpg`:

```{r}
# Visualize predictions and logistic model
mtcars %>%
  # Make predictions
  mutate(predictions = predict(fit_log, type = "response")) %>%
  ggplot() +
  # Draw the logistic model
  geom_smooth(aes(x = mpg, y = am),
              method = "glm", se = FALSE, 
              method.args = list(family = "binomial")) + 
  # Draw original data
  geom_point(aes(x = mpg, y = am)) +
  # Draw predictions
  geom_point(aes(x = mpg, y = predictions), color = "orange")
```

To split the predictions into 0 or 1 (automatic or manual), we would have to decide on a cutoff value for the probability of the transmission being manual. For example, let's try the cutoff value 0.5. 

#### **Try it! Create a new variable called `predicted` that takes a value of 1 if the transmission is likely to be manual and 0 if not. Redo the graph above and color the original data points by the predicted value for the transmission. Which points on the graph shows: True positive, True negative, False positive, False negative?**

```{r}
# Write code here
mtcars %>%
  # Make predictions
  mutate(predictions = predict(fit_log, type = "response")) %>%
  mutate(predicted = ifelse(predictions > 0.5, 1, 0)) %>%
  ggplot() +
  # Draw the logistic model
  geom_smooth(aes(x = mpg, y = am),
              method = "glm", se = FALSE, 
              method.args = list(family = "binomial")) + 
  # Draw original data
  geom_point(aes(x = mpg, y = am, color = as.factor(predicted))) +
  # Draw predictions
  geom_point(aes(x = mpg, y = predictions), color = "orange")

```

We can also make predictions for new data. For example, my new car is supposed to get a fuel consumption of about 30 mpg:

```{r}
# Make predictions for new data
rav4 <- data.frame(mpg = 30)
predict(fit_log, newdata = rav4, type = "response")
```

The model predicts a probability of 93% for the transmission to be manual. But my car has an automatic transmission. Why is the prediction so much in error?

### d. Performance

We can evaluate the performance of our classifier (classifying the transmission as either automatic or manual) by looking at the accuracy (what proportion of cars were classified with the correct transmission?).

#### **Try it! Evaluate the performance of our classifier using accuracy.**

```{r}
# Write code here
mtcars %>%
  # Make predictions
  mutate(predictions = predict(fit_log, type = "response")) %>%
  mutate(predicted = ifelse(predictions > 0.5, 1, 0)) %>%
  mutate(countvar = ifelse(predicted == am, TRUE, FALSE)) %>%
  select(mpg, am, predictions, predicted, countvar) %>%
  summarize(mean(countvar))

```

We previously discussed another measure of performance with the ROC curve and the corresponding area under the curve (AUC):

```{r}
# ROC curve
ROC <- mtcars %>%
  # Make predictions
  mutate(predictions = predict(fit_log, type = "response")) %>%
  ggplot() + 
  geom_roc(aes(d = am, m = predictions), n.cuts = 10)
ROC
```

```{r}
# Calculate the area under the curve
calc_auc(ROC)$AUC
```

Our classifier seems to perform fairly well. What would happen if we changed our cutoff value to 0.4 instead of 0.5?

## 3. Using multiple predictors

What if we add another predictors to our model?

```{r}
# Fit the model using two predictors
fit_log <- glm(am ~ mpg + wt, data = mtcars, family = "binomial")

# Take a look at the model summary
summary(fit_log)
```

How has our model improved?

```{r}
# ROC curve
ROC <- mtcars %>%
  # Make predictions
  mutate(predictions = predict(fit_log, type = "response")) %>%
  ggplot() + 
  geom_roc(aes(d = am, m = predictions), n.cuts = 10)
ROC
```

```{r}
# Calculate the area under the curve
calc_auc(ROC)$AUC
```

The value of the AUC is higher so our model is making better predictions!

Important note: we can add many variables to our model but if we have more variables compared to the number of observations, the model might artificially work out:

```{r}
# Fit the model using all predictors
fit_log <- glm(am ~ ., data = mtcars, family = "binomial")

# Take a look at the model summary
summary(fit_log)
```

The output said the algorithm did not converge...

## Your turn!

Let's practice the classification methods of logistic regression to predict the malignancy of a tumor in the `biopsy` dataset:

```{r}
# Upload the data from GitHub
biopsy <- read_csv("https://raw.githubusercontent.com/laylaguyot/datasets/main//Biopsy.csv")

# Take a quick look at the data
head(biopsy)
```

1.  Overwrite the variable `outcome` in `biopsy` as taking value 1 if malignant, 0 if benign.

```{r}
# Try it!
biopsy <- biopsy %>%
  mutate(outcome = ifelse(outcome == "malignant", 1, 0))
  
```

2.  Choose one variable to use as the predictor and use the function `glm()` to fit a logistic regression model. Build a ROC curve. What cutoff value should we choose for the probability of being malignant? What does the value of AUC tell us about our classifier?

```{r}
# Try it!
fit_log <- glm(outcome ~ uniform_cell_size, data = biopsy, family = "binomial")

ROC <- biopsy %>%
  # Make predictions
  mutate(predictions = predict(fit_log, type = "response")) %>%
  ggplot() + 
  geom_roc(aes(d = outcome, m = predictions), n.cuts = 10)
ROC

calc_auc(ROC)$AUC
```

3.  Try another predictor and fit a new logistic regression model. Build another ROC curve for this model. How do the two models compare?

```{r}
# Try it!
fit_log <- glm(outcome ~ bare_nuclei, data = biopsy, family = "binomial")

ROC <- biopsy %>%
  # Make predictions
  mutate(predictions = predict(fit_log, type = "response")) %>%
  ggplot() + 
  geom_roc(aes(d = outcome, m = predictions), n.cuts = 10)
ROC

calc_auc(ROC)$AUC

```

4.  Post your ROC plots on the following slideshow and include your code in the comments: <https://docs.google.com/presentation/d/14-hZIGTr7M0Bryr0ktjxXq6bSseBJFhmBIoey1Q2oHo/edit?usp=sharing>
