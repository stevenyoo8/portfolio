---
title: "Worksheet 14: Linear Regression"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r global_options, include=FALSE}
# The following code is a basic setup of options for your document
# You won't have to edit it (unless you want to!)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE,  
                      warning = FALSE, message = FALSE, 
                      fig.align = "center",
                      R.options = list(max.print = 100))

# Edit the file starting below
```

## 1. Set up

We will only use the `tidyverse` package in this worksheet.

```{r, message = FALSE}
# Load packages
library(tidyverse)
```

The dataset we will explore today is the built-in R object called `mtcars`:

```{r}
# Looks familiar?
head(mtcars)
```

This dataset contains information about different features of some cars. We will try to predict the `mpg` (response variable) based on other features (predictor variable(s)).

## 2. Predicting a numeric response with a numeric predictor

First, let's try to predict the `mpg` based on the weight of a car `wt`.

### a. Exploring relationships

#### **Try it! Use `ggplot` to represent the relationship between `mpg` and `wt`. Does there appear to be a relationship?**

```{r}
# scatterplot
mtcars %>%
  ggplot(aes(x = wt, y = mpg)) +
  geom_point()

cor(mtcars$mpg, mtcars$wt)
```

### b. Fitting a model

If we suspect a strong linear relationship between two variables, we can consider a linear regression model:

```{r}
# Fitting a linear model
ggplot(data = mtcars, aes(x = wt, y = mpg)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Predictor: Weight (1,000 lbs)", 
       y = "Response: Fuel consumption (mpg)")
```

To find the linear model represented above, we use the `lm(response ~ predictor, data = ...)` function:

```{r}
# Fit the model
fit_lin <- lm(mpg ~ wt, data = mtcars)

# Take a look at the model summary
summary(fit_lin)
```

The model shows that we can predict the `mpg` of a car if we know its weight `wt` as follows:

$\widehat{mpg} = 37.2851 - 5.3445 * wt$

We use the hat to specify that we get predicted values of mpg (as opposed to `mpg`, the actual observations).

### c. Making predictions

Let's use the expression of the model to make predictions.

#### **Try it! Use the expression of the model to create a new variable called `predictions` that predicts values of `mpg` based on values of `wt`.**

```{r}
# Write code here
mtcars %>%
  mutate(predictions = 37.2851 - (5.3445 * wt)) %>%
  select(wt, mpg, predictions)
```

Or much more convenient: using the `predict()` function:

```{r}
# Calculate a predicted value
mtcars %>% 
  mutate(predictions = predict(fit_lin)) %>%
  select(wt, mpg, predictions)
```

We can also make predictions for new data. For example, I just bought a new car, a Toyota RAV4 that weighs about 3,500 lbs:

```{r}
# Make predictions for new data
rav4 <- data.frame(wt = 3.5)
predict(fit_lin, newdata = rav4)
```

The predicted fuel consumption is about 18.6 mpg... Well... my new car is supposed to get a much better fuel consumption. This is an example of what we call **extrapolation**: we use a model that is not applicable to our new data. The cars contained in `mtcars` were listed in the 1974 Motor Trend US magazine.

However, we don't usually make 100% accurate predictions because there is some variation in our data. Compare these two cars for example:

```{r}
# Same weight, different mpg
mtcars %>%
  filter(wt == 3.57) %>%
  select(wt, mpg)
```

### d. Residuals

Our predictions don't usually match exactly what we observed. Let's calculate residuals which represent the difference between actual values and predicted values:

```{r}
# Calculate residuals
mtcars %>% 
  mutate(predictions = predict(fit_lin)) %>% # predictions based on model
  mutate(residuals = mpg - predictions) %>%
  select(wt, mpg, predictions, residuals)
```

Or using the `resid()` function:

```{r}
# Residuals
mtcars %>% 
  mutate(residuals = resid(fit_lin)) %>%
  select(wt, mpg, residuals)
```

Let's visualize the residuals:

```{r}
mtcars %>% 
  # Save predictions
  mutate(predictions = predict(fit_lin)) %>% 
  # Use a ggplot to represent the relationship
  ggplot(aes(x = wt, y = mpg)) +
  # Add the linear model
  geom_smooth(method = "lm", se = FALSE) + 
  # Add residuals = vertical segments from observations to predictions
  geom_segment(aes(xend = wt, yend = predictions), alpha = .2) +
  # Display the observed data (on top of the line and segments)
  geom_point() +
  # Display the predictions
  geom_point(aes(y = predictions), color = "orange")
```

### e. Performance

To quantify performance for linear regression models, we usually consider the average distance between the predicted values from the model and the actual values in the dataset. This is called the root mean square error (RMSE) of the model.

```{r}
# Calculate RMSE of regression model
sqrt(mean(resid(fit_lin)^2))
```

The lower the RMSE, the better a model fits a dataset.

Or we can also consider the coefficient of determination $R^2$, which reports the percentage of variation in the response variable that can be explained by the predictor variables.

```{r}
# Calculate R^2 of regression model
summary(fit_lin)$r.squared

# indicates variation of about 25%
```

The higher the $R^2$, the better a model fits a dataset.

#### **Try it! Predict `mpg` based on another feature of the car like `disp`. Which predictor (`wt` or `disp`) provided a better model?**

```{r}
# Write code here
fit_lin_disp <- lm(mpg ~ disp, data = mtcars)

sqrt(mean(resid(fit_lin_disp)^2))
summary(fit_lin_disp)$r.squared

# or both values in summary
summary(fit_lin_disp)

```

## 3. Using a categorical predictor

What if we chose to predict the fuel consumption based on the transmission of a car (automatic vs manual)?

Let's take a look at the relationship with this new predictor:

```{r}
# Represent the relationship 
ggplot(mtcars, aes(fill = as.factor(am), y = mpg)) +
  geom_boxplot()

# What would be a better geom to use here?
```

We can fit a linear regression model with the `am` predictor:

```{r}
# Fit the model
fit_lin <- lm(mpg ~ am, data = mtcars)

# Take a look at the model summary
summary(fit_lin)
```

#### **Try it! Write the expression of the new model. Predict values of `mpg` based on `am` using `predict()`. Why does it make sense to get what we get? And what are we getting exactly?**

$\widehat{mpg} = 7.245 * am + 17.147$

```{r}
# Write code here
mtcars %>%
  mutate(predictions = 7.245 * am + 17.147) %>%
  select(am, mpg, predictions)

mtcars %>%
  group_by(am) %>%
  summarize(mean(mpg))
```

## 4. Using multiple predictors

Actually we can add many predictors to our linear regression model! What if we combine `wt` and `am`?

```{r}
# Fit the model
fit_lin <- lm(mpg ~ wt + am, data = mtcars)

# Take a look at the model summary
summary(fit_lin)
```

Has the performance of our model improved?

And what if we add all possible predictors?

```{r}
# Fit the model using all predictors (that make sense)
fit_lin <- lm(mpg ~ ., data = mtcars)

# Take a look at the model summary
summary(fit_lin)
```

We get a very complex model but we improved the performance!

Note:

-   Adding too many variables can create issues such as **overfitting**: the model is too specific to the cars in the dataset on which we "train" the model and it will be very difficult to generalize to other cars.

-   We can quickly check which features might be more useful for making predictions by looking at the last column in the model output. Any `.` or `*` shows which features are "significant".

-   To compare models with a different number of predictors, we compare the values of the Adjusted $R^2$. The adjustment takes into account that we added more predictors which naturally inflates the value of $R^2$.
