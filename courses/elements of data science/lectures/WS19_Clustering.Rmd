---
title: "Worksheet 19: Clustering"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r global_options, include=FALSE}
# The following code is a basic setup of options for your document
# You won't have to edit it (unless you want to!)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE,  
                      warning = FALSE, message = FALSE, 
                      fig.align = "center",
                      R.options = list(max.print = 100))

# Edit the file starting below
```

## 1. Set up

New package for today is `cluster`. 

```{r, eval=FALSE}
# Install new packages (only needed once!)
install.packages("cluster")
```

We will also use the `tidyverse` and `factoextra` packages.

```{r, message=FALSE}
# Load packages
library(tidyverse)
library(factoextra)
library(cluster)
```

Remember the `msleep` dataset? It contains information about 

```{r}
# Take a quick look
head(msleep)
```

## 2. Distances

We will consider distances to measure how "far" observations are from each other. For example, let's compare cows to dogs in terms of their total amount of sleep and body weight:

```{r}
# Filter and select
cow_dog <- msleep %>%
  filter(name %in% c("Cow","Dog")) %>%
  select(sleep_total, bodywt)
cow_dog
```

In the slides, we first defined 2 different types of distances. Let's calculate them by hand first to understand where they come from:

```{r}
# Calculate distances by hand for sleep total
cow_dog %>%
            # Euclidean = sqrt of the squared differences
  summarize(euclidean = sqrt(diff(sleep_total)^2 + diff(bodywt)^2),
            # Manhattan = absolute value of the differences
            manhattan = abs(diff(sleep_total)) + abs(diff(bodywt)))
```

Much easier to just use the R function called `dist()`:

```{r}
# Calculate distances with the R function dist()
dist(cow_dog, method = "euclidean")
dist(cow_dog, method = "manhattan")
```

There was a big difference in body weight so it might a good idea to scale our variables (compute z-score) because of the different units/scales our variables can have.

#### **Try it! Scale the variables (before filtering for Cow and Dog). Then calculate the Euclidean distance between Cow and Dog. How does that distance compare to the distance between Cow and Asian elephant?**

```{r}
# Write code here
msleep %>%
   select(name, sleep_total, bodywt) %>%
   mutate(sleep_total_scaled = scale(sleep_total),
          bodywt_scaled = scale(bodywt)) %>%
   filter(name %in% c("Cow","Dog")) %>%
  select(sleep_total_scaled, bodywt_scaled) %>%
  dist()

msleep %>%
   select(name, sleep_total, bodywt) %>%
   mutate(sleep_total_scaled = scale(sleep_total),
          bodywt_scaled = scale(bodywt)) %>%
   filter(name %in% c("Cow","Asian elephant")) %>%
  select(sleep_total_scaled, bodywt_scaled) %>%
  dist()

```

The goal of clustering is to identify observations that are alike. We will discuss two algorithms.

## 3. k-means clustering

The 4 steps in k-means clustering are to:

1.  Pick *k* points of the *n* observations at random to serve as initial cluster centers.

2.  Assign each *n-k* observation to the cluster whose center is closest.

3.  For each group, calculate means and use them as new centers.

4.  Repeat steps 2-3 until groups stabilize.

### a. Prepare the data

Let's focus on 2 variables in the `msleep` dataset, `sleep_toal` and `bodywt`. It is a good practice to scale our variables before clustering (because we are calculating distances to perform the clustering).

```{r}
# Keep two variables and scale them
msleep_scaled <- msleep %>% 
  select(sleep_total, bodywt) %>%
  scale
```

### b. Apply the algorithm

Let's use the `kmeans` function with `k = 2` to find 2 clusters:

```{r}
# Use the function kmeans() to find clusters
kmeans_results <- msleep_scaled %>%
  kmeans(centers = 2) # centers sets the number of clusters to find

# Take a look at the resulting object
kmeans_results
```

We will focus on the `centers` and `cluster` objects:

```{r}
# Summary about the centers
kmeans_results$centers

# A vector attributing a cluster number to each observation
kmeans_results$cluster
```

What else can we do with these clusters?

### c. Visualize the clusters

We can save the identification of the cluster for each observation in the dataset to manipulate the observations for each cluster:

```{r}
# Save cluster assignment as a column in the original dataset
msleep %>%
  mutate(cluster = as.factor(kmeans_results$cluster)) %>%
  head
```

#### **Try it! Using `ggplot()`, visualize the relationship between `sleep_total` and `bodywt` with the cluster assignment of each mammal.**

```{r}
# Write code here
msleep %>%
  mutate(cluster = as.factor(kmeans_results$cluster)) %>%
  ggplot(aes(x = sleep_total, y = bodywt, color = as.factor(cluster))) +
  geom_point()
```

We can visualize the resulting clusters more directly with the function `fviz_cluster()`.

```{r}
# Let's visualize our data with cluster assignment
fviz_cluster(kmeans_results, data = msleep_scaled)
```

### d. Interpreting the clusters

What characteristics do the mammals share in each cluster? We can create summary statistics of each variable for each cluster to understand some characteristics about the clusters.

#### **Try it! Using `dplyr` functions, find the mean value of the 2 variables of interest for each cluster.**

```{r}
# 
msleep %>%
  mutate(cluster = as.factor(kmeans_results$cluster)) %>%
  group_by(cluster) %>%
  summarize(sleep_mean = mean(sleep_total),
            bodywt_mean = mean(bodywt))
```

### e. Number of clusters

Determining the number of clusters to use can be tricky. We will consider the average silhouette width (which measures how cohesive and separated clusters are, simultaneously) for multiple values of `k`. A high average silhouette width indicates a good clustering.

```{r}
# Maximize the silhouette while keeping a small number of clusters
fviz_nbclust(msleep_scaled, kmeans, method = "silhouette")
```

The average silhouette width seems to indicate that 6 clusters maximize the average width silhouette.

#### **Try it! Find the 6 clusters with `kmeans` based on the `sleep_total` and `bodywt` variables in the `msleep_scaled` data. Visualize the clusters. How different are the clusters?**

```{r}
# Write code here
kmeans_results <- msleep_scaled %>%
  kmeans(centers = 6) # centers sets the number of clusters to find

# Take a look at the resulting object
kmeans_results

fviz_cluster(kmeans_results, data = msleep_scaled)
```

### f. Including more variables

What if we would like to consider more variables to compare the mammals? Let's keep all the numeric variables, omit missing values, and scale the variables:

```{r}
# Keep all numeric variables and scale them
msleep_num_scaled <- msleep %>% 
  select_if(is.numeric) %>%
  na.omit %>%
  scale
```

Then check how many clusters we should keep:

```{r}
# Maximize the silhouette while keeping a small number of clusters
fviz_nbclust(msleep_num_scaled, kmeans, method = "silhouette")
```

While the average silhouette width indicates that we should consider 3 clusters, there is not much difference with only keeping 2 clusters.

```{r}
# Use the function kmeans() to find clusters
kmeans_results <- msleep_num_scaled %>%
  kmeans(centers = 2)
```

Visualize the clusters. Why was PCA performed?

```{r}
# Let's visualize our data with cluster assignment
fviz_cluster(kmeans_results, data = msleep_num_scaled)
```

*Note: we could recover the principal components with `prcomp()`.*

What characteristics do the mammals share within each cluster? Let's describe each cluster:

```{r}
# Create basic summary statistics for each cluster in original units
msleep %>%
  select_if(is.numeric) %>% 
  na.omit %>%
  mutate(cluster = as.factor(kmeans_results$cluster)) %>%
  group_by(cluster) %>%
  summarize_all(mean)
```

## 4. PAM clustering

Another algorithm to do clustering is called the Partitioning around Medoids (PAM). The 4 steps in PAM clustering are to:

1.  Pick *k* points of the *n* observations at random to serve as initial cluster centers.

2.  Assign each *n-k* observation to the cluster whose center is closest.

3.  For each group, assign a new central observation that minimizes the sum of the distances to the center.

4.  Repeat steps 2-3 until groups stabilize.

### a. Number of clusters

How many clusters should we look for if considering all numeric variables with non missing values in `msleep`?

```{r}
# Determine the number of clusters based on the silhouette width
fviz_nbclust(msleep_num_scaled, pam, method = "silhouette")
```

### b. Apply the algorithm

We use the `pam` function to find two clusters based on numeric variables in the `msleep` dataset:

```{r}
# Use the function pam() to find clusters
pam_results <- msleep_num_scaled %>%
  pam(k = 3) # k is the number of clusters

# Take a look at the resulting object
pam_results
```

The PAM algorithm identifies one of the mammals at the center (rather than an average). They're called the medoids. Which mammals are at the center?

Visualize the clusters, again using PCA:

```{r}
# Let's visualize the clusters after dimension reduction
fviz_cluster(pam_results, data = msleep_num_scaled)
```

## 5. Clustering with categorical variables

Let's discuss another example including a categorical variable with the dataset `msleep`.

```{r}
msleep_reduced <- msleep %>%
  # Drop some categorical variables with too many categories)
  select(-name,-genus,-order,-conservation) %>%
  # Consider categorical variables as factors
  mutate_if(is.character, as.factor) %>%
  # Ignore missing values
  na.omit
```

### a. Gower's distance

This new measure allows us to find distances using categorical variables too! We can access the function `daisy()` to calculate this new measure in the package `cluster`.

```{r}
library(cluster)
```

Let's find the pairwise Gower's dissimilarity for all variables in `msleep_reduced`:

```{r}
# Calculate Gower distances between observations
msleep_reduced %>%
  # No need to scale when calculating the Gower's distance
  daisy(metric = "gower") %>%
  # Save as a matrix
  as.matrix -> msleep_gower
```

We obtained a matrix that contains all pairwise distances (distances between two mammals).

### b. Clustering

Let's apply the PAM algorithm to the "dissimilarity" object (the output containing the different distances that we called `msleep_gower`). How many clusters should we look for?

```{r}
# Use the silhouette on the matrix of distances
fviz_nbclust(msleep_gower, pam, method = "silhouette")
```

The advantage of the PAM algorithm is that we can perform clustering on a dissimilarity object (the matrix of the distances) with the argument `diss = TRUE`.

```{r}
# Apply PAM on the dissimilarity object (specify diss = TRUE)
pam_results <- pam(msleep_gower, k = 2, diss = TRUE)
```

Let's interpret what each cluster represents.

```{r}
# Summary statistics of numeric variables
msleep_reduced %>%
  mutate(cluster = as.factor(pam_results$clustering)) %>%
  group_by(cluster) %>%
  summarize_if(is.numeric, mean, na.rm = T)
```

```{r}
# Summary statistics of the categorical variable vore
msleep_reduced %>%
  mutate(cluster = as.factor(pam_results$clustering)) %>%
  group_by(cluster, vore) %>%
  summarize(freq = n()) 

# How to improve this display?
```

Which mammal is representative of each cluster? Look at the center of each cluster:

```{r}
# Look at the final medoids
msleep[pam_results$id.med,]
```